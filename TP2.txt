Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. In case of doubt, you can use PyCharm as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.
- You can include references to images or html files such as the reports generated with clusters. To do this, simply include this document in the folder with the reports or images and refer them in the text by the file name in an isolated line. For example, the line

test.png

refers to a test.png image file in the same folder as this document.

QUESTIONS:

Q1: Explain how you selected the best attributes for the clustering phase. In particular, the visualization methods used to explore the 18 extracted attributes and any statistical tests used.
A1: To select the best attributes for clustering, we employed PCA, t-SNE and Isomap. These methods help visualize and reduce the dimensionality of our data while retaining important features, which later would be concatenated together. PCA highlights linear relationships, t-SNE reveals non-linear patterns, and Isomap preserves the data's geometry. By transforming our data into lower-dimensional spaces using these techniques, we identified which combinations of attributes provided the most distinct clusters visually. Additionally, statistical tests like ANOVA were utilized to quantify the relevance of each attribute to our clustering goals, ensuring we focused on the most informative features for accurate cluster formation.


Q2: After selecting the attributes, did you standardize or normalize the values? Justify your decision.
A2: Values did were normalized. Using for this the fit_transform() method in each feature extractor. This step was crucial to ensure that each attribute contributed equally to the clustering process. By doing so, attributes with larger numerical ranges do not dominate the clustering algorithm, which could otherwise bias the results towards those attributes. 

Q3: How is the value of the neighborhood radius (epsilon) determined for the DBSCAN algorithm as described in the article "A density-based algorithm for discovering clusters in large spatial databases with noise"?
A3: 
The value of the neighborhood radius (ε) in the DBSCAN algorithm is determined based on the density of the data points. It reflects how far DBSCAN should search to find densely connected points, defined by the MinPts parameter, which specifies the minimum number of neighbors a point must have within ε to be considered a core point. To find ε, DBSCAN evaluates the distribution of distances to nearest neighbors in the dataset, aiming to set it such that it encapsulates the majority of densely clustered points while excluding sparse areas. Adjusting ε ensures DBSCAN effectively identifies clusters of varying shapes and sizes in the data.


Q4: Examining the clusters generated by the DBSCAN algorithm with the value optimized by the method described in the article, do you think the result is suitable for clustering these images? Justify your response.
A4: We could observed, as explained in the comment at the end of the main.py file, that the precision scores reached in the DBSCAN clustering we really low, none of the reaching the 50%, so with such a high miss-clasification, the algorithm isn't the most suitable one for this dataset


Q5: Describe your analysis of the parameters k (for K-Means) and epsilon (for DBSCAN) using the internal and external indicators mentioned in the statement. Include the two charts with the values of the indicators (indicating the name of each plot image in a line of the response) as a function of the k and epsilon parameters, and describe the choice of the ranges in which you examined these parameters. Indicate, justifying, what conclusions you can draw from this analysis.
A5: Based on the evaluation of K-Means and DBSCAN clustering algorithms using different numbers of features, we observed varying performance across different metrics. The charts depicting the Rand Index Score and Adjusted Rand Index Score as functions of kkk (for K-Means) and ϵ\epsilonϵ (for DBSCAN) can provide insights into the algorithm's suitability for clustering the images.
For K-Means clustering, the Rand Index Score generally increases with kkk, indicating better similarity to the ground truth labels. Adjusted Rand Index Score follows a similar trend, suggesting improved clustering quality with higher kkk. In contrast, DBSCAN shows consistent Rand Index Scores across different ϵ\epsilonϵ values, indicating its sensitivity to parameter tuning. Adjusted Rand Index Scores for DBSCAN remain low, reflecting its struggle to match clusters with ground truth labels effectively.

NOTES: The graphs are included on the results folder created when executing the code, both for rand index and the adjusted rand index

Q6: Select some of the parameters tested in question five and examine the corresponding clusters more closely, generating the HTML file with the images. Justify the choice of these values, discuss the different options, and propose a recommendation that could help biologists in classifying cells and rejecting segmentation errors.
A6: NOTES: the html files are located on the folders k_means, dbscan and affinity_propagation
Evaluated metrics and their values:
-precision score: measures the proportion of the clasified instances as correct which really are correctly classified
 ·increase: the number of false positives is decreeses (failed classifications decrease)
 ·decrease: the number of false positives is increased (failed classifications increase)
-recall score: measures the proportion of the positive instances which were correctly classified
 ·incremento: the model identifies correctly more positive instances
 ·decrease: the model identifies correctly less positive instances
-F1 Score: combination of precition and recall
 ·increase: increases with the increase of precition/recall
 ·decrease: decreeases with the decreease of precition/recall
-Rand Index Score: evaluates the similarity for the clustering and the reference partition
 ·increase: when similarity increases, rand index increases
 ·decrease: when similarity decreases, rand index decreases
- Adjusted Rand index score: rand index modification which considers the aleatoriety when evaluating the similarity
 ·increase: same than rand index
 ·decrease: same than rand index
so then, we will look for the highest parameters possible

Recommendations for Biologists
Procedure for Processing Segmented Images

Image Preprocessing:
Convert to grayscale, normalize, and apply filters to reduce noise.

Feature Extraction:
Use PCA, t-SNE, and Isomap to obtain a combination of morphological and texture-based features.

Feature Selection:
Based on the results, select between 4 and 6 features for an optimal balance of precision and stability.

Clustering:
K-Means Clustering:
Preferred method due to superior performance in precision and Rand Index.
Use between 4 and 6 features for clustering.

DBSCAN Clustering:
Not recommended due to inconsistency and poor performance in key metrics.
May be useful as a supplementary method to detect noise or missegmented images.

Evaluation and Validation:
Calculate metrics such as precision, recall, F1-score, and Rand Index to assess clustering quality.
Utilize Adjusted Rand Index for a more robust evaluation of clustering quality.

Cell Classification and Error Discard:
Use clusters generated by K-Means for cell classification.
Manually review images classified as noise by DBSCAN to confirm segmentation errors and adjust the model if necessary.

To assist biologists in processing segmented images, the primary recommendation is to use K-Means clustering (chosen over DBSCAN due to significantly higher precision and other metrics, increasing from below 40% to nearly 70% precision). Select between 4 to 6 features, which generally exhibit higher values across evaluated metrics (precision score, recall, etc.). Working with 5 or 6 features shows a notable increase in precision compared to 4 features (from 57% to 68% precision), albeit with a slight decrease in recall score. Therefore, determining a middle ground between these cases is ideal. This method has proven to be more accurate and consistent in cell classification for the models used.
 

Q7: Discuss advantages, problems, or other aspects of these two algorithms (K-Means and DBSCAN) that you consider relevant to help biologists organize these images, considering your theoretical knowledge of these algorithms as well as the results you obtained in your work.
A7: Based on the evaluation results across different numbers of features (1 to 7), DBSCAN consistently struggles with achieving high precision and Adjusted Rand Index Scores, indicating challenges in handling noisy data and varying cluster densities typical in biological images. In contrast, K-Means demonstrates a progressive increase in precision and Rand Index Scores as more features are evaluated, reflecting its effectiveness in identifying well-separated clusters with clear centroids. Affinity Propagation shows mixed performance, achieving high precision in some cases but generally lower recall, suggesting potential over-clustering tendencies. Choosing the optimal algorithm for organizing biological images should consider these trade-offs, focusing on the dataset's noise levels, cluster density variations, and the need for interpretable clustering results.


Q8 (Optional): Consider other clustering algorithms implemented in the Scikit-Learn library (such as Affinity). Choose one and apply it to this problem, optimizing the parameters as you see fit in the best way possible. Justify your choice and discuss whether this option would yield more useful results for biologists.
A8: The affinity propagation, tends to have a high precision, even higher than the kmeans (for 5 features kmeans have a 57% precision, vs the 67% offered by the affinity), but when observing the recall score affinity is at 11% which is a 30% lower than the kmeans for just a 10% increase in precision. So as the importance isn't just in the precision, but on the balance between the parameters, the recomendation of classifying with K-Means is mantained. Althougth the depending on what the cell classification is used for, the affinity could be recommended.This cases are those were missclassifying a positive is not that important. This cases are those were we just wanna reject a diagnostic but we aren't looking foward confirming the diagnostic with it
ex: in types of cancers, we know that the volume of cells is increased, so we could use the classification to determine if the cells aren't growing on volume, using for this the highest precision posible, as later in case the cells are growing on size, we will still have to do a second test to know if  the tumor is benigne or a cancer
